{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Symbol encoding\n",
    "\n",
    "### How it works?\n",
    "\n",
    "* We can compress a sequence of symbols if each one is translated by a code-word and,\n",
    "  in average, the lengths of the code-words are smaller than the\n",
    "  length of the symbols.\n",
    "  \n",
    "* The encoder and the decoder have a probabilistic model $M$ which\n",
    "  provides to a variable-length encoder ($C$)/decoder($C^{-1}$) the\n",
    "  probability $p(s)$ of each symbol $s$.\n",
    "  \n",
    "* The most probable symbols are represented by the shorter\n",
    "  code-words and viceversa.\n",
    "\n",
    "<img src=\"data/compresion_entropica.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bits, data and information\n",
    "\n",
    "* data != information (data is the representation of the information).\n",
    "\n",
    "* Lossless data compression uses a shorter representation for\n",
    "  information.\n",
    "  \n",
    "* By definition, a bit of data stores a bit of information, if and\n",
    "  only if, it represents the occurrence of an equiprobable event (an\n",
    "  event that can be true or false with the same probability).\n",
    "  In this ideal situation, the representation is fully efficient\n",
    "  (no futher compression would be possible).\n",
    "  \n",
    "* By definition, a symbol $s$ with probability $p(s)$ stores\n",
    "  \\begin{equation}\n",
    "    I(s)=-\\log_2 p(s) \\tag{Eq:symbol_information}\n",
    "  \\end{equation}\n",
    "  bits of information.\n",
    "\n",
    "  <img src=\"data/prob_vs_long.png\" width=\"600\">\n",
    "\n",
    "* So, ideally, the length of a code-word in bits (of data) should match with the number bits of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entropy\n",
    "\n",
    "* The entropy $H(S)$ measures the amount of information per\n",
    "  symbol that a source of information $S$ produces, in average. By definition\n",
    "  \\begin{equation}\n",
    "    H(S) = \\frac{1}{N}\\sum_{s=1}^{N} p(s)\\times I(s)\n",
    "  \\end{equation}\n",
    "  bits-of-information/symbol, where $N$ is the size of the source\n",
    "  alphabet (number of different symbols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Basic compression algorithm\n",
    "\n",
    "#### Encoding of a symbol\n",
    "\n",
    "1. While the decoder does not know the symbol:\n",
    "    1. Assert something about the symbol that allows to the decoder\n",
    "    to minimize the uncertainty of finding that symbol. This guess\n",
    "    should have true or false with the same probability.\n",
    "    2. Output a bit of code that says if the last guess is true or\n",
    "    false.\n",
    "    \n",
    "#### Decoding of a symbol\n",
    "\n",
    "1. While the symbol is not known without uncertainty:\n",
    "    1. Make the same guess that the encoder.\n",
    "    2. Input a bit of code that represents the result of the last\n",
    "    guess.\n",
    "    \n",
    "#### Tip\n",
    "\n",
    "* This codec is 100% efficient if the guesses are equiprobable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Let's suppose that we use the Spanish alphabet. Humans know that\n",
    "  symbols does not form words in any order, so we can\n",
    "  formulate the following VLC (Variable Length Codec):\n",
    "  \n",
    "  In Spanish there are 28 letters. Therefore, to encode, for example,\n",
    "  the word `preciosa`, the first symbol `p` can be represented by\n",
    "  its index inside of the Spahish alphabet with a code-word of 5 bits. In\n",
    "  this try, the encoding is not a very efficient, but this we are in first\n",
    "  letter ... For the second one `r` we can see (using a\n",
    "  Spanish dictionary) that after a `p`, the following symbols are\n",
    "  possible: (1) `a`, (2) `e`, (3) `i`, (4) `l`, (5) `n`, (6)\n",
    "  `o`, (7) `r`, (8) `s` and (9) `u`. Therefore, we don't need\n",
    "  5 bits now, 4 are enough.\n",
    "  \n",
    "<img src=\"data/universal_coding_example.png\" width=\"600\">\n",
    "\n",
    "* Notice that the compression ratio has been 40/25:1 (`preciosa` has 8\n",
    "  letters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. [Shannon-Fano coding](https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding) [[Shannon, 1948]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Shannon+2001+A+Mathematical+Theory+of+Communication&btnG=),  [[Fano, 1949]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Fano+1949+%22The+transmission+of+information%22&btnG=)\n",
    "\n",
    "* At the end of the 40's, Claude E. Shannon (Bell Labs) and\n",
    "  R.M. Fano (MIT) developed the Shannnon-Fano codec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. Sort the symbols using their probabilities.\n",
    "2. Split the set of symbols into two subsets in a way in which the\n",
    "   each subset have the same total probability.\n",
    "3. Assign a different bit to each set.\n",
    "4. Repeat the previous procedure to each subset until  each subset\n",
    "   has only one symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Let's use the following probabilistic model:\n",
    "\n",
    "<img src=\"data/shannon-fano_example.png\" width=\"180\">\n",
    "\n",
    "Using it, this is the Shannon-Fano coding:\n",
    "\n",
    "<img src=\"data/shannon-fano_example-coding.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "TO-DO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding) [[Huffman, 1952]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=huffman+method+codes+1952&btnG=)\n",
    " \n",
    "* (Absolute) Optimal performance (in average, better than Shannon-Fano) when a integer\n",
    "  number of bits is assigned to each symbol.\n",
    "* Huffman-based VLC codecs build a binary tree where the symbols are stored\n",
    "  in the leafs and the distance of each symbol to the root of the tree\n",
    "  is $\\lceil\\log_2(p(s))\\rceil$.\n",
    "* After label each binary branch in the tree, the Huffman\n",
    "  code-word for the symbol $s$ is the sequence of bits (labels) that\n",
    "  we must use to travel from the root to the $s$-leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Building Huffman trees\n",
    "\n",
    "1. Create a list of nodes. Each node stores a symbol and its\n",
    "   probability.\n",
    "2. While the number of nodes in the list > 1:\n",
    "    1. Extract from the list the 2 nodes with the lowest probability.\n",
    "    2. Insert in the list a new node (that is the root of a binary\n",
    "       tree) whose probability is the sum of the probability of its\n",
    "       leafs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"data/huffman_ejemplo.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Limits\n",
    "\n",
    "* Any Huffman code satisfies that\n",
    "  \\begin{equation}\n",
    "    l\\big(c(s)\\big) = \\lceil I(s)\\rceil, \\tag{Eq:Huffman}\n",
    "  \\end{equation}\n",
    "  where $l\\big(c(s)\\big)$ is the length of the code-word assigned to\n",
    "  the symbol $s$. This implies that, with each encoded symbol, up to 1 bit of\n",
    "  redundant data can be introduced (think about a very frequent -- high probability -- symbol).\n",
    "  \n",
    "* This is a problem that grows when the size of the alphabet is\n",
    "  small. In the extreme case, for binary source alphabets, the Huffman\n",
    "  coding does not change the length of the original representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Arithmetic coding\n",
    "\n",
    "* Arithmetic coding relaxes the Eq. (Eq:Huffman),\n",
    "  verifying that, for every encoded symbol, \n",
    "  \\begin{equation}\n",
    "    l\\big(c(s)\\big) = I(s), \\tag{Eq:arithmetic}\n",
    "  \\end{equation}\n",
    "  i.e. the number of bits of data (code-word) assigned by the encoder\n",
    "  is equal to the number of bits of information that the symbol\n",
    "  represent.\n",
    "\n",
    "<img src=\"data/comparacion.png\" width=\"800\">\n",
    "\n",
    "* It can be said that, arithmetic coding is optimal because\n",
    "  the average length of an arithmetic code is equal to the entropy of\n",
    "  the information source, measured in bits/symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An ideal encoder\n",
    "\n",
    "1. Let $[L,H)\\leftarrow [0.0,1.0)$ an interval of real numbers.\n",
    "2. While the input is not exhausted:\n",
    "    1. Split $[L,H)$ into so many sub-intervals as different symbols\n",
    "       are in the alphabet. The size of each sub-interval is proportional\n",
    "       to the probability of the corresponding symbol.\n",
    "    2. Select the sub-interval $[L',H')$ associated with the encoded\n",
    "       symbol.\n",
    "    3. $[L,H)\\leftarrow [L',H')$.\n",
    "3. Output a real number $x\\in[L,H)$ (the arithmetic\n",
    "   code-stream). The number of decimals of $x$ should be large enough\n",
    "   to distinguish the final sub-interval $[L,H)$ from the rest of\n",
    "   possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Imagine a binary sequence, where $p(\\text{A})=3/4$ and\n",
    "  $p(\\text{B})=1/4$. Compute the arithmetic code of the sequences A, B,\n",
    "  AA, AB, BA y BB.\n",
    "  \n",
    "<img src=\"data/aritmetica_ejemplo.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An ideal decoder\n",
    "\n",
    "1. Let $[L,H)\\leftarrow [0.0,1.0)$ the initial interval.\n",
    "2. While the input is not exhausted:\n",
    "    1. Split $[L,H)$ in so many sub-intervals as different symbols\n",
    "       are in the alphabet. The size of each sub-interval is proportional\n",
    "       to the probability of the corresponding symbol.\n",
    "    2. Input so many bits of $x$ as they are needed to:\n",
    "        1. Select the sub-interval $[L',H')$ that contains $x$.\n",
    "        2. Output the symbol that $[L',H')$ represents.\n",
    "        3. $[L,H)\\leftarrow[L',H')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Incremental transmission\n",
    "\n",
    "* It is not necessary to wait for the end of the encoding to\n",
    "  generate the arithmetic code. When we work with binary\n",
    "  representations of the real numbers $L$ and $H$, their most\n",
    "  significant bits become identical when the interval is reduced. These\n",
    "  bits belong to the output arithmetic code, therefore, they\n",
    "  can be output as soon as they match.\n",
    "  \n",
    "  For example, when the symbol B is encoded, a code-bit 1 can be\n",
    "  output because any sequence of symbols that start with B have a\n",
    "  code-word that begins with 1.\n",
    "    \n",
    "* When the most significant bits of $L$ and $H$ are output, the\n",
    "  bits of each register are shifted to the left, and new bits need to\n",
    "  be inserted. The results is an automatic zoom of the selected\n",
    "  sub-interval.\n",
    "\n",
    "  Following with the previous example, the register shifting generates\n",
    "  an ampliation of the $[0.50,1.00)$ interval to the $[0.00,1.00)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab\n",
    "TO-DO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Probabilistic models\n",
    "\n",
    "* In order to use any of the previous VLCs, a probabilistic model is always needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1 Static models\n",
    "\n",
    "* Static models are the simplest ones because they suppose that the probabilities of the symbols\n",
    "  remain constant.\n",
    "* In this case, variable-length code-words can be precomputed.\n",
    "* Static models are very common in codecs such\n",
    "  as JPEG, MPEG (audio and video), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.2 Adaptive models\n",
    "\n",
    "* The probabilities of the symbols are computed in run-time.\n",
    "* In general, the compression ratios that adaptive models\n",
    "  get are better than static model's ones, because the\n",
    "  probabilities of the symbols are localy computed\n",
    "  (think of the sequence `aaaaaaaaaaaaaabbbbbbbbbbbbbbb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoding\n",
    "\n",
    "1. Asign the same probability to all the symbols.\n",
    "2. While the input if not exhausted:\n",
    "    1. Encode the next symbol.\n",
    "    2. Update (increase) its probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoding\n",
    "\n",
    "1. Identical to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. Decode the next symbol.\n",
    "    2. Identical to the step 2.B of the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5.3 Initially empty models\n",
    "\n",
    "* The smaller the number of symbols used by the model, the higher\n",
    "  the probabilities, and therefore, the better the compression ratios.\n",
    "* An initially empty model only stores the ESC(cape) symbol, a\n",
    "  symbol that it is used by the encoder only when a new symbol is\n",
    "  found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. Set the probability of the $\\text{ESC}$ to $1.0$ (and the probability of\n",
    "   the rest of the symbols to $0.0$).\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next symbol.\n",
    "    2. If $s$ has been found before, then:\n",
    "        1. Output $c(s)$ (encode).\n",
    "    3. Else:\n",
    "        1. Output $c(\\text{ESC})$.\n",
    "        2. Output a raw symbol $s$.\n",
    "    4. Update $p(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "1. Identical to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c(s)\\leftarrow $ next code-word.\n",
    "    2. Decode $s$.\n",
    "    3. If $s=\\text{ESC}$, then:\n",
    "        1. Input a raw symbol $s$.\n",
    "    4. Update $p(s)$.\n",
    "    5. Output $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   \n",
    "## 5.4 Models with memory\n",
    "\n",
    "* In most cases, the probability of a symbol depends on its\n",
    "  neighborhood (context).\n",
    "* The higher the memory of the model (the context), the higher the\n",
    "  accuracy of the predictions (probabilities), and therefore, the\n",
    "  lower the length of the code-words \\cite{Cleary.PPM}.\n",
    "* Let ${\\cal C}[i]$ the last $i$ encoded symbols and\n",
    "  $p(s|{\\cal C}[i])$ the probability that the symbol $s$ follows\n",
    "  the context ${\\cal C}[i]$.\n",
    "* Let $k$ the maximal order of the prediction (i.e. the largest\n",
    "  number of symbols of ${\\cal C}[]$ that are going to be used as the\n",
    "  actual context). Notice that ${\\cal C}[0]=\\varnothing$ and the model\n",
    "  has no memory.\n",
    "* We suppose that arithmetic coding is used and therefore, when we\n",
    "  input or output $c(s)$, we are transmitting $I(s)$ bits of code.\n",
    "* Let $r$ the size of the source alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. Create an empty model for every context $0\\le i \\le k$.\n",
    "2. Create an non-empty model for $k=-1$.\n",
    "3. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ Input$_{\\log_2(r)}$.\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where\n",
    "       $i\\leftarrow 0$).\n",
    "    3. While $p(s|{\\cal C}[i])=0$ (it is the first time that $s$ follows\n",
    "       ${\\cal C}[i]$):\n",
    "        1. Output $\\leftarrow c(\\text{ESC}|{\\cal C}[i])$.\n",
    "        2. Update $p(\\text{ESC}|{\\cal C}[i])$.\n",
    "        3. Update $p(s|{\\cal C}[i])$ (insert $s$ into the ${\\cal C}[i]$ context).\n",
    "        4. $i\\leftarrow i-1$.\n",
    "    4. Output $\\leftarrow c(s|{\\cal C}[i])$. The symbols that were in\n",
    "       contexts with order $>i$ must be excluded of the actual (${\\cal C}[i]$) context because $s$ is not none of them.\n",
    "    5. If $i\\ge 0$, update $p(s|{\\cal C}[i])$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Let $r=256$ the size of the source alphabet.\n",
    "\n",
    "* The probabilistic model $M[{\\cal C}[-1]]$ (for the special context\n",
    "  ${\\cal C}[-1]$) is non adaptative, non empty and has an special symbol EOF\n",
    "  (End Of File) that is going to be used when the compression has\n",
    "  finished:\n",
    "  $$M[{\\cal C}[-1]]=\\{0,1~1,1~\\cdots~\\mathtt{a},1~\\mathtt{b},1~\\cdots~255,1~\\text{EOF},1\\}.$$\n",
    "  In a pair $a,b$, $a$ is the symbol and $b$ is its probability (counts).\n",
    "\n",
    "* $M[{\\cal C}[0]]$ is adaptative and empty:\n",
    "  $$M[{\\cal C}[0]]=\\{\\text{ESC},1\\}.$$\n",
    "\n",
    "* In this example (for the sake of the simplicity), the maximal\n",
    "  order of the prediction $k=1$ (we only remember the previous\n",
    "  symbol). Therefore, there are $r=256$ probabilistic models:\n",
    "  $$M[{\\cal C}[1]]=\\{\\text{ESC},1\\}, 0\\le {\\cal C}[1]\\le r.$$\n",
    "  \n",
    "* Encoding of the first symbol (\\texttt{a}) (see Figure~\\ref{fig:PPM}):\n",
    "\n",
    "1. [3.A] $s\\leftarrow$ \\texttt{a}.\n",
    "2. [3.B] $i\\leftarrow 0$ (we don't know the previous symbol).\n",
    "3. [3.C] $p(\\mathtt{a}|{\\cal C}[0])=0$ (the context only has the ESC).\n",
    "4. [3.C.a] Output $\\leftarrow c(\\text{ESC}|{\\cal C}[0])$ (althought\n",
    "    $l(c(\\text{ESC}|{\\cal C}[0]))=0$).\n",
    "5. [3.C.b] Update $p(\\text{ESC}|{\\cal C}[0])$ (now, the count of ESC is\n",
    "    2).\n",
    "6. [3.C.c] Insert \\texttt{a} into\n",
    "    $M[{\\cal C}[0]]=\\{\\mathsf{ESC},2~\\mathtt{a},1\\}$.\n",
    "7. [3.C.d] $i\\leftarrow -1$.\n",
    "8. [3.c] $p(\\mathtt{a}|{\\cal C}[-1])\\neq 0$.\n",
    "9. [3.d] Output $\\leftarrow c(\\texttt{a}|{\\cal C}[-1])$ where\n",
    "    $p(\\texttt{a}|{\\cal C}[-1]) = 1/(256+1)$.\n",
    "    \n",
    "* Encoding of the second symbol (\\texttt{b}):\n",
    "\n",
    "1. [3.a] $s\\leftarrow$ \\texttt{b}.\n",
    "2. [3.b] $i\\leftarrow 1$.\n",
    "3. [3.c] $p(\\mathtt{b}|{\\cal C}[1])=0$ because ${\\cal C}[1]=\\texttt{a}$ and\n",
    "   $M[\\texttt{a}]=\\{\\text{ESC},1\\}$.\n",
    "4. [3.c.i] Output $\\leftarrow c(\\text{ESC}|\\texttt{a})$ (althought\n",
    "   $l(c(\\text{ESC}|\\texttt{a}))=0$).\n",
    "5. [3.c.ii] Update $p(\\text{ESC}|\\texttt{a})$ (now, the count of ESC is 2).\n",
    "6. [3.c.iii] Insert \\texttt{b} into $M[\\texttt{a}]=\\{\\text{ESC},2~ \\texttt{b},1\\}$.\n",
    "7. [3.c.iv] $i\\leftarrow 0$.\n",
    "8. [3.c] $p(\\mathtt{b}|{\\cal C}[0])=0$ because\n",
    "   $M[{\\cal C}[0]]=\\{\\mathsf{ESC},2~\\texttt{a},1\\}$.\n",
    "9. [3.c.i] Output $\\leftarrow c(\\text{ESC}|{\\cal C}[0])$ where\n",
    "   $p(\\text{ESC}|{\\cal C}[0]) = 2/3$.\n",
    "10. [3.c.ii] Update $p(\\text{ESC}|{\\cal C}[0])$ (now, the count of ESC is\n",
    "    3).\n",
    "11. [3.c.iii] Insert \\texttt{b} into $M[{\\cal C}[0]] = \\{\\text{ESC},3~\n",
    "    \\texttt{a},1~ \\texttt{b},1\\}$.\n",
    "12. [3.c.iv] $i\\leftarrow -1$.\n",
    "13. [3.c] $p(\\mathtt{b}|{\\cal C}[-1])\\neq 0$.\n",
    "14. [3.d] Output $\\leftarrow c(\\texttt{b}|{\\cal C}[-1])$ where\n",
    "    $p(\\mathtt{b}|{\\cal C}[-1]) = 1/r$. The symbol \\texttt{a} has been\n",
    "    excluded in the calculus of the probability of \\texttt{b} because\n",
    "    $\\texttt{a}\\in M[{\\cal C}[0]] = \\{\\text{ESC},3~ \\texttt{a},1~\n",
    "    \\texttt{b},1\\}$.\n",
    "\n",
    "<img src=\"data/PPM_example.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "1. Equal to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    2. $s\\leftarrow$ next decoded symbol.\n",
    "    3. While $s=\\text{ESC}$:\n",
    "        1. Update $p(\\text{ESC}|{\\cal C}[i])$.\n",
    "        2. $i\\leftarrow i-1$.\n",
    "        3. $s\\leftarrow$ next decoded symbol.\n",
    "    4. Update $p(s|{\\cal C}[i])$.\n",
    "    5. While $i<k$:\n",
    "        1. $i\\leftarrow i+1$.\n",
    "        2. Update $p(s|{\\cal C}[i])$ (insert $s$ into the ${\\cal C}[i]$ context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. MTF (Move To Front) transform\n",
    "\n",
    "* Inputs a sequence of symbols and outputs a sequence of symbols.\n",
    "\n",
    "* The size (in bits of data) for each sequence is the same.\n",
    "\n",
    "* The entropy of the output is lower that the input's one.\n",
    "\n",
    "* Performs a change in the representation of the symbols where\n",
    "  those symbols that have a high probability of occurrency are\n",
    "  \"moved\" in the source alphabet towards decreasing positions.\n",
    "\n",
    "* The probability density function follows an exponential\n",
    "  distribution with a slope $\\lambda$ where\n",
    "  \\begin{equation}\n",
    "    f(x.\\lambda) = \\left\\{ \\begin{array}{ll}\n",
    "      \\lambda e^{-\\lambda x} & \\mbox{if $x \\geq 0$};\\\\\n",
    "      0 & \\mbox{if $x < 0$}.\\end{array} \\right.\n",
    "  \\end{equation}\n",
    "\n",
    "<img src=\"data/exponential.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Forward transform\n",
    "\n",
    "1. Create a list $L$ with the symbols of the source alphabet\n",
    "  where $$L[s]\\leftarrow s; 0\\le s\\le r.$$\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next input symbol.\n",
    "    2. $c\\leftarrow$ position of $s$ in $L$ ($L[c]=s$).\n",
    "    3. Output $\\leftarrow c$.\n",
    "    4. Move $s$ to the front of $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "Not copied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inverse transform\n",
    "\n",
    "1. The step 1 of the forward transform.\n",
    "2. While the input is not exausted:\n",
    "    1. $c\\leftarrow$ next input code.\n",
    "    2. $s\\leftarrow L[c]$.\n",
    "    3.  Output $s$.\n",
    "    4. The step 2.C of the forward transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"data/MTF_example.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.7 Context-based Text Predictive transform\n",
    "\n",
    "* The MTF uses a model where a symbol that has happened only one\n",
    "  time can get a index-code that is lower than the index-code of a\n",
    "  symbol that has been found thousands of times :-(\n",
    "\n",
    "* We can solve this problem if the positions of the symbols are\n",
    "  determined by their probability. In other words, the list $L$ will\n",
    "  be sorted by the ocurrence of the symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 0-order encoder\n",
    "\n",
    "1. The step 1 of the MTF transform, although now every node of the\n",
    "   list stores also a count of the symbol.\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next input symbol.\n",
    "    2. $c\\leftarrow$ position of $s$ in $L$ (the prediction error).\n",
    "    3. Output $\\leftarrow c$.\n",
    "    4. Update the count of $L[c]$ (the count of $s$) and keep sorted $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"data/TPT_example.svg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 0-order decoder\n",
    "\n",
    "1. The step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c\\leftarrow$ next input code.\n",
    "    2. $s\\leftarrow L[c]$.\n",
    "    3. Output $s$.\n",
    "    4. Step 2.D of the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $N$-order encoder\n",
    "\n",
    "1. Let ${\\cal C}[i]$ the context of $s$ and $L_{{\\cal C}[i]}$ the\n",
    "   list for that context. If $i>0$ then the lists are empty, else, the\n",
    "   list is full and the count of every node is $0$.\n",
    "2. Let $N$ the order of the prediction.\n",
    "3. Let $H=\\varnothing$ a list of tested symbols. All symbols in $H$\n",
    "   must be different.\n",
    "4. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ the next input symbol.\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    3. While $s\\notin L_{{\\cal C}[i]}$:\n",
    "        1. $H\\leftarrow \\text{reduce}(H\\cup L_{{\\cal C}[i]})$. (reduce$()$ deletes the repeated nodes).\n",
    "        2. Update the count of $s$ in $L_{{\\cal C}[i]}$ and keep sorted it.\n",
    "        3. $i\\leftarrow i-1$.\n",
    "    4. Let $c$ the position of $s$ en $L_{{\\cal C}[i]}$.\n",
    "    5. $c\\leftarrow c+$ symbols of $H-L_{{\\cal C}[i]}$. In this\n",
    "       way, the decoder will know the length of the context where $s$\n",
    "       happens and does not count the same symbol twice.\n",
    "    6. Output $\\leftarrow c$.\n",
    "    7. Update the count of $s$ in $L_{{\\cal C}[i]}$ and keep sorted it.\n",
    "    8. $H\\leftarrow\\varnothing$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example ($k=1$)\n",
    "\n",
    "<img src=\"data/TPT_example.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### $N$-order decoder\n",
    "\n",
    "1. Steps 1, 2 and 3 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c\\leftarrow$ the next input code.\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    3. While $L_{{\\cal C}[i]}[c]=\\varnothing$:\n",
    "        1. $H\\leftarrow \\text{reduce}(H\\cup L_{{\\cal C}[i]})$.\n",
    "        2. $i\\leftarrow i-1$.\n",
    "    4. $s\\leftarrow \\text{reduce}(H\\cup L_{{\\cal C}[i]})[c]$.\n",
    "    5. Update the count of $L_{{\\cal C}[i]}[c]$.\n",
    "    6. While $i<k$:\n",
    "        1. $i\\leftarrow i+1$.\n",
    "        2. Insert the symbol $s$ in $L_{{\\cal C}[i]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.8 Unary coding\n",
    "\n",
    "* It is a particular case of the Huffman code where the number of\n",
    "  bits of each code-word (minus one) is equal to the index of the\n",
    "  symbol in the source alphabet. Example:\n",
    "  \n",
    "<img src=\"data/Unary_example.png\" width=\"150\">\n",
    "\n",
    "* The unary coding is only optimal when (see Equation\n",
    "  \\ref{eq:symbol_information})\n",
    "  \\begin{equation}\n",
    "    p(s) = 2^{-(s+1)} \\tag{Eq:Unary}\n",
    "  \\end{equation}\n",
    "  where $s=0,1,\\cdots$.\n",
    "  \n",
    "<img src=\"data/unary.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.9 Golomb coding [[Golomb, 1966]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=golomb+1966+run&btnG=)\n",
    "\n",
    "* When the probabilities of the symbols follow an exponential\n",
    "  distribution, the Golomg encoder has the same efficiency than the\n",
    "  Huffman coding, but it is faster. In this case, the probabilities of\n",
    "  the symbols shoud be\n",
    "  \\begin{equation}\n",
    "    p(s) =\n",
    "    2^{\\displaystyle-\\Big(\\displaystyle m\\big\\lfloor\\displaystyle\\frac{s+m}{m}\\big\\rfloor\\Big)}\n",
    "    \\tag{Eq:Golomb}\n",
    "  \\end{equation}\n",
    "  where $s=0,1,\\cdots$ is the symbol and $m=0,1,\\cdots$ is the\n",
    "  \"Golomb slope\" of the distribution.\n",
    "  \n",
    "* For $m=2^k$, it is possible to find a very efficient\n",
    "  implementation and the encoder is also named Rice\n",
    "  encoder~\\cite{Rice79}. In this case\n",
    "  \\begin{equation}\n",
    "    p(s) =\n",
    "    2^{\\displaystyle-\\Big(2^k \\displaystyle\\big\\lfloor\\displaystyle\\frac{s+2^k}{2^k}\\big\\rfloor\\Big)}\n",
    "    \\tag{Eq:Rice}\n",
    "    \\label{eq:Rice}\n",
    "  \\end{equation}\n",
    "\n",
    "<img src=\"data/Golomb_coding.png\" width=\"600\">\n",
    "\n",
    "* Notice that for $m=1$, we take the unary encoding.\n",
    "\n",
    "<img src=\"data/Golomb.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. $k\\leftarrow \\lceil\\log_2(m)\\rceil$.\n",
    "2. $r\\leftarrow s~\\mathrm{mod}~m$.\n",
    "3. $t\\leftarrow 2^k-m$.\n",
    "4. Output $(s~\\mathrm{div}~m)$ using an unary code.\n",
    "5. If $r<t$:\n",
    "    1. Output the integer encoded in the $k-1$ least significant bits of $r$ using a binary code.\n",
    "6. Else:\n",
    "    1. $r\\leftarrow r+t$.\n",
    "    2. Output the integer encoded in the $k$ least significant bits of $r$ using a binary code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example ($m=7$ and $s=8$)\n",
    "\n",
    "1. [1] $k\\leftarrow \\lceil\\log_2(8)\\rceil=3$.\n",
    "2. [2] $r\\leftarrow 8 \\text{mod} 7 = 1$.\n",
    "3. [3] $t\\leftarrow 2^3-7 = 8-7 = 1$.\n",
    "4. [4] Output $\\leftarrow 8 \\text{div} 7 = \\lfloor 8/7\\rfloor=1$ as an unary code (2 bits). Therefore, output $\\leftarrow 10$.\n",
    "5. [5] $r=1\\le t=1$.\n",
    "6. [6.A] $r\\leftarrow 1+1=2$.\n",
    "7. [6.B] Output $r=2$ using a binary code of $k=3$ bits. Therefore, $c(8)=10010$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "1. $k\\leftarrow\\lceil\\log_2(m)\\rceil$.\n",
    "2. $t\\leftarrow 2^k-m$.\n",
    "3. Let $s\\leftarrow$ the number of consecutive ones in the input (we stop when we read a $0$).\n",
    "4. Let $x\\leftarrow$ the next $k-1$ bits in the input.\n",
    "5. If $x<t$:\n",
    "    1. $s\\leftarrow s\\times m+x$.\n",
    "6. Else:\n",
    "    1. $x\\leftarrow x\\times 2~+$ next input bit.\n",
    "    2. $s\\leftarrow s\\times m+x-t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example (decode $10010$ where $m=7$)\n",
    "\n",
    "1. [1] $k\\leftarrow 3$.\n",
    "2. [2] $t\\leftarrow 2^k-m = 2^3-7=1$).\n",
    "3. [3] $s\\leftarrow 1$ because we found only one $1$ in the input.\n",
    "4. [4] $x\\leftarrow \\text{input}_{k-1} = \\text{input}_2 = 01$.\n",
    "5. [5] $x=1\\nless t=1$.\n",
    "6. [6.A] $x\\leftarrow x\\times x\\times 2+\\text{next input bit} = x\\times 1\\times 2+0 = 2$.\n",
    "7. [6.B] $s\\leftarrow s\\times m+x-t = 1\\times 7+2-1=8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Lab\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## C.10 Rice coding\n",
    "\n",
    "* Between Unary and Golomb coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. $m\\leftarrow 2^k$.\n",
    "2. Output $\\leftarrow\\lfloor s/m\\rfloor$ using an unary code ($\\lfloor s/m\\rfloor+1$ bits).\n",
    "3. Output $\\leftarrow$ the $k$ least significant bits of $s$ using a binary code.\n",
    "\n",
    "### Example ($k=1$ and $s=7$)\n",
    "1. [1] $m\\leftarrow 2^k=2^1=2$.\n",
    "2. [2] Output $\\leftarrow \\lfloor s/m\\rfloor=\\lfloor 7/2\\rfloor=3$ using an unary code of 4 bits. Therefore, output $\\leftarrow 1110$.\n",
    "3. Output $\\leftarrow$ the $k=1$ least significant bits of $s=7$\n",
    "  using a unary code ($k+1$ bits). So, output $\\leftarrow 1$. Total\n",
    "  output $c(7)=11101$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "1. Let $s$ the number of consecutive ones in the input (we stop when we read a 0).\n",
    "2. Let $x$ the next $k$ input bits.\n",
    "3. $s\\leftarrow s\\times 2^k+x$.\n",
    "\n",
    "### Example (decode $11101$ where $k=1$)\n",
    "1. [1] $s\\leftarrow 3$ because we found $3$ consecutive ones in the input.\n",
    "2. [2] $x\\leftarrow$ next input $k=1$ input bits. Therefore $x\\leftarrow 1$.\n",
    "3. [3] $s\\leftarrow s\\times 2^k+x = 3\\times 2^1+1 = 6+1 = 7$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab\n",
    "TO-DO"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
